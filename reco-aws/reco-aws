#!/bin/bash

set -ex
set -o pipefail

export DIR
DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"

GENERATE_AFI=${GENERATE_AFI:-no}

function help {
    echo "

reco-aws $VERSION

usage: $0 <command>

Where <command> is one of the following:

    Testing:

    test <cmd>              Test the current kernel using a hardware simulator

    Building:

    build                   Build the project in the current folder.

    check-afi <build_id>    Check the state of the build's output AFI.

    afi-logs <build_id>     Retrieve logs from AFI generation.

    Other:

    help    Display this help output
"
}

function bundle {
    mkdir -p .reco-work
    tar hczf .reco-work/bundle.tar.gz --exclude .reco-work ./* 1>&2
}

function copy_bundle {
    aws s3 cp --quiet .reco-work/bundle.tar.gz "s3://reconfigureio-builds/tmp/$1.tar.gz" 1>&2
}

function clean_bundle {
    aws s3 rm "s3://reconfigureio-builds/tmp/$1.tar.gz" 1>&2
}

function wait_for_batch {
    while :
    do
        status=$(aws batch describe-jobs --jobs "$1" | jq -r ".jobs[0].status")
        case $status in
           "SUCCEEDED")
                echo "0"
                break
                ;;
            "FAILED")
                echo "1"
                break
                ;;
            "null")
                echo "1"
                break
                ;;
            *)
                sleep 60s
                ;;
        esac
    done

}

function wait_for_afi {
    # If we aren't generating an AFI, we can't wait for it
    if [ "$GENERATE_AFI" = "yes" ]; then
        while :
        do
            AFI=$(afi_id_from_build_id "$1")
            status=$(aws ec2 describe-fpga-images --fpga-image-ids "$AFI" | jq -r ".FpgaImages[0].State.Code")
            case $status in
                "available")
                    echo "0"
                    break
                    ;;
                "failed")
                    echo "1"
                    break
                    ;;
                "null")
                    echo "1"
                    break
                    ;;
                *)
                    sleep 60s
                    ;;
            esac
        done
    else
        echo "0"
    fi
}

function submit_job {
    SUBMIT=$(cat "$DIR/../aws/job_submit.json" | jq ".containerOverrides.command[0] = \"$2\" | .containerOverrides.environment[2].value = \"s3://reconfigureio-builds/tmp/$1.tar.gz\" | .containerOverrides.environment[7].value = \"s3://reconfigureio-builds/tmp/$1.dist.zip\" | .containerOverrides.environment[4].value = \"${@:3}\" | .containerOverrides.environment[8].value = \"tmp/dcp/$1.tar\" | .containerOverrides.environment[9].value = \"tmp/dcp-logs/$1\" | .containerOverrides.environment[10].value = \"$GENERATE_AFI\"")
    aws batch submit-job --cli-input-json "$SUBMIT" | jq -r ".jobId"
}

function submit_deploy {
    AGFI="$1"
    NUMBER="$2"
    CMD="${@:3}"
    SUBMIT=$(cat "$DIR/../aws/deploy_test.json" | jq ".containerOverrides.environment[2].value = \"$AGFI\" | .containerOverrides.environment[0].value = \"$CMD\" | .containerOverrides.environment[1].value = \"s3://reconfigureio-builds/tmp/$NUMBER.dist.zip\"")
    aws batch submit-job --cli-input-json "$SUBMIT" | jq -r ".jobId"
}

function stream_batch_logs {
    LOG_STREAM=$(aws batch describe-jobs --jobs "$1" | jq -r ".jobs[0].container.logStreamName")
    stream_logs "$LOG_STREAM"
}

function stream_logs {
    STREAM=$(aws logs describe-log-streams --log-group-name /aws/batch/job --log-stream-name-prefix "$1" | jq -r ".logStreams[0].logStreamName")
    aws logs get-log-events --log-group-name /aws/batch/job --log-stream-name "$STREAM" | jq -r ".events | .[] | .message" 1>&2
}

function afi_id_from_build_id {
    while :
    do
        AFI=$(aws s3 ls s3://reconfigureio-builds/tmp/dcp-logs/"$1"/ | grep -o 'afi-[^ ,]\+')
        if [ -z "$AFI" ]
        then
            sleep 60s
        else
            echo "${AFI::-1}"
            break
        fi
    done
}

function get_afi_logs {
    AFI=$(afi_id_from_build_id "$1")
    LOGFILE=$(aws s3 ls s3://reconfigureio-builds/tmp/dcp-logs/"$1"/"$AFI"/ | grep "\.log" | awk '{print $4}' | head -n1)
    aws s3 cp s3://reconfigureio-builds/tmp/dcp-logs/"$1"/"$AFI"/"$LOGFILE" -
}

function generate_job_id {
    JOB=$(uuidgen -t)
    echo "$JOB"
}

function run_batch {
    JOB="$2"
    bundle
    copy_bundle "$JOB"
    JOB_ID=$(submit_job "$JOB" "$1" "${@:3}")
    CODE=$(wait_for_batch "$JOB_ID")
    clean_bundle "$JOB"
    stream_batch_logs "$JOB_ID"
    echo "$CODE"
}

function run_deploy {
    AFI=$(afi_id_from_build_id "$1")
    AGFI=$(aws ec2 describe-fpga-images --fpga-image-ids "$AFI" | jq -r ".FpgaImages[0].FpgaImageGlobalId")
    JOB_ID=$(submit_deploy "$AGFI" "$@")
    CODE=$(wait_for_batch "$JOB_ID")
    stream_logs "staging-deploy/$JOB_ID"
    exit "$CODE"
}

function build {
    JOB_ID=$(generate_job_id)
    echo "$JOB_ID"
    CODE_BUILD=$(run_batch "/opt/build.sh" "$JOB_ID" "${@:1}")
    if [ "$CODE_BUILD" == "0" ]
    then
        CODE_AFI=$(wait_for_afi "$JOB_ID")
        exit "$CODE_AFI"
    else
        exit "$CODE_BUILD"
    fi
}

function test-cmd {
    JOB_ID=$(generate_job_id)
    CODE=$(run_batch "/opt/simulate.sh" "$JOB_ID" "${@:1}")
    exit "$CODE"
}

function main {
    case "$1" in
        "test")
            test-cmd "${@:2}"
            ;;
        "build")
            build "${@:2}"
            ;;
        "run")
            run_deploy "${@:2}"
            ;;
        "check-afi")
            wait_for_afi "${@:2}"
            ;;
        "afi-logs")
            get_afi_logs "${@:2}"
            ;;
        *)
            echo "unrecognized command: $1";
            help "$@"
            exit 1
            ;;
        esac
}

main "$@"
